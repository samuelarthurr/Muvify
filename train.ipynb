# src/models/svm_classifier.py

from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
import numpy as np
from typing import Tuple, List, Dict
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

class ViewerClassifier:
    def __init__(self):
        # Initialize SVM classifier with RBF kernel
        self.svm = SVC(
            kernel='rbf',
            C=1.0,  # Regularization parameter
            gamma='scale',  # Kernel coefficient
            probability=True,
            random_state=42
        )
        self.scaler = StandardScaler()
        self.segments = ['casual', 'regular', 'enthusiast']
        
    def train(self, features: np.ndarray) -> None:
        """
        Train the SVM classifier using viewer features
        
        Parameters:
        features (np.ndarray): Array of [watch_time, completion_rate, rating_pattern]
        """
        # Scale features for better performance
        scaled_features = self.scaler.fit_transform(features)
        
        # Create initial labels using basic thresholds
        labels = self._create_initial_labels(scaled_features)
        
        # Train SVM
        self.svm.fit(scaled_features, labels)
    
    def predict(self, features: np.ndarray) -> List[str]:
        """
        Predict viewer segment for given features
        """
        scaled_features = self.scaler.transform(features)
        predictions = self.svm.predict(scaled_features)
        return [self.segments[p] for p in predictions]
    
    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Dict[str, float]:
        """
        Evaluate model performance using cross-validation
        """
        # Perform k-fold cross validation
        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        
        metrics = {
            'accuracy': [],
            'precision': [],
            'recall': [],
            'f1': []
        }
        
        for train_idx, test_idx in kf.split(X):
            X_train, X_test = X[train_idx], X[test_idx]
            y_train, y_test = y[train_idx], y[test_idx]
            
            # Train model
            self.svm.fit(X_train, y_train)
            
            # Make predictions
            y_pred = self.svm.predict(X_test)
            
            # Calculate metrics
            metrics['accuracy'].append(accuracy_score(y_test, y_pred))
            metrics['precision'].append(precision_score(y_test, y_pred, average='weighted'))
            metrics['recall'].append(recall_score(y_test, y_pred, average='weighted'))
            metrics['f1'].append(f1_score(y_test, y_pred, average='weighted'))
        
        # Calculate average metrics
        return {k: np.mean(v) for k, v in metrics.items()}
    
    def _create_initial_labels(self, scaled_features: np.ndarray) -> np.ndarray:
        """
        Create initial labels based on engagement scores
        """
        engagement_scores = scaled_features[:, 1]  # Use engagement score feature
        labels = np.zeros(len(engagement_scores), dtype=int)
        
        # Set thresholds for segmentation
        lower_threshold = np.percentile(engagement_scores, 33)
        upper_threshold = np.percentile(engagement_scores, 66)
        
        # Assign labels based on thresholds
        labels[(engagement_scores >= lower_threshold) & 
               (engagement_scores < upper_threshold)] = 1
        labels[engagement_scores >= upper_threshold] = 2
        
        return labels
    
    def generate_confusion_matrix(self, X_test: np.ndarray, y_test: np.ndarray) -> np.ndarray:
        """
        Generate confusion matrix for model evaluation
        """
        y_pred = self.svm.predict(X_test)
        return confusion_matrix(y_test, y_pred)